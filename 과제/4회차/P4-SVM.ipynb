{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [SWCON253] Machine Learning\n",
    "Teaching Assistant: Yeongwoong Kim (duddnd7575@khu.ac.kr)\n",
    "\n",
    "Professor: Jinwoo Choi (jinwoochoi@khu.ac.kr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P4.A:  SVM을 이용하여 선형 분류 (8점)\n",
    "\n",
    "### 학습목표\n",
    "- SVM 모델을 이용하여 선형 분류 가능한 데이터를 분류하는 분류기를 학습 할 수 있다.\n",
    "- Scikit-Learn을 이용하여 모델 학습, 검증 과정을 구현할 수 있다.\n",
    "\n",
    "### 실습내용\n",
    "Scikit-Learn의 SVM을 이용하여 선형 분류가 가능한 데이터셋을 분류하는 분류기를 학습해봅니다. \n",
    "\n",
    "실습은 다음 순서로 진행됩니다.\n",
    "- 1) 주어진 데이터셋 loading\n",
    "- 2) SVM 모델 구현, 학습 **<직접 구현>**\n",
    "- 3) SVM 모델 검증\n",
    "- 4) Decision Boundary 시각화 \n",
    "- 5) SVM with soft margin **<직접 구현>**\n",
    "- 6) Discussion\n",
    "\n",
    "**이번 실습에서 여러분은 `2), 5)` 부분의 코드를 직접 작성합니다.**\n",
    "\n",
    "앞으로 대부분의 실습도 위와 같은 순서로 진행됩니다. 이번 실습을 통해 각 부분의 코드를 이해하고 다음 실습에 참고하도록합니다.\n",
    "\n",
    "\n",
    "### 점수\n",
    "- SVM 모델 작성: 8점\n",
    "- 본 실습과제에서는 Discussion 성실도에 따른 감점 최대 2점\n",
    "\n",
    "`.ipynb 파일과 함께 .html 파일 (File -> export as -> HTML)도 함께 제출하세요. 하나만 제출할시 감점이 있습니다.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "No module named 'sklearn' 에러 발생 시 `conda install scikit-learn` 명령어를 통해 scikit-learn 패키지를 설치해주시기 바랍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Loading the Dataset\n",
    "코드를 실행시켜 실습코드와 같이 첨부된 dataset.csv파일을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일을 읽기\n",
    "X, y = [], []\n",
    "\n",
    "with open('./dataset.csv', 'r') as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            values = line.split(',')\n",
    "        else:\n",
    "            continue\n",
    "        X.append([float(i) for i in values[:2]])\n",
    "        y.append(int(values[-1]))\n",
    "        \n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Split & Visualization\n",
    "Load 된 데이터셋을 모델 학습과 검증을 위해 Trainset과 Testset으로 랜덤 샘플링하여 나누고 데이터셋이 어떤 분포로 생겼는지 시각화하여 살펴봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "# 랜덤시드 설정\n",
    "random.seed(1234)\n",
    "\n",
    "# 데이터 랜덤 셔플\n",
    "idx = list(range(len(X)))\n",
    "random.shuffle(idx)\n",
    "\n",
    "# 앞 80개 까지는 학습용으로 뒤 20개는 테스트용으로 split\n",
    "X_train = [X[i] for i in idx[:80]]\n",
    "y_train = [y[i] for i in idx[:80]]\n",
    "X_test = [X[i] for i in idx[80:]]\n",
    "y_test = [y[i] for i in idx[80:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 별로 데이터를 시각화 하여 분포를 살펴보기\n",
    "plt.scatter([i[0] for idx, i in enumerate(X_train) if y_train[idx] == 0], \n",
    "            [i[1] for idx, i in enumerate(X_train) if y_train[idx] == 0],\n",
    "            label='class 0', marker='o')\n",
    "\n",
    "plt.scatter([i[0] for idx, i in enumerate(X_train) if y_train[idx] == 1], \n",
    "            [i[1] for idx, i in enumerate(X_train) if y_train[idx] == 1],\n",
    "            label='class 1', marker='s')\n",
    "\n",
    "plt.title('Training set')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.xlim([0.0, 7])\n",
    "plt.ylim([-0.8, 0.8])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) SVM 모델 구현, 학습\n",
    "이 부분에서는 Scikit-Learn의 SVC 모듈의 **linear** 커널을 이용하여 선형 SVM모델을 선언하고 학습합니다.\n",
    "\n",
    "여기서 사용하는 SVC 클래스는 C-Support Vector Classification으로 SVM을 사용한 분류기 모델입니다.\n",
    "\n",
    "SVC class에서는 kernel 파라미터를 이용하여 사용하는 커널의 형태를 변경할 수 있습니다. 이번 실습에서는 **'linear'(선형)** 커널을 사용한 선형 SVM을 이용합니다.\n",
    "\n",
    "아래 `# <your code>` 부분을 채워 넣어서 코드를 직접 구현하세요.\n",
    "\n",
    "**세부 구현 사항:**\n",
    "- kernel은 'linear'\n",
    "- Hard margin을 사용하기 위해 하이퍼파라미터 C=100으로 설정\n",
    "\n",
    "**유의사항:**\n",
    "- SVC 모듈의 하이퍼파라미터인 C는 강의 시간에 배운 하이퍼파라미터 C와 유사하나, SVC 모듈의 경우 C가 커질 수록 Hard margin에 가까워짐\n",
    "- 자세한 설명(하이퍼파라미터, 학습방법)은 scikit-learn의 공식 document를 참조 바랍니다. \n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='linear', C=100)\n",
    "##### <your code> training with train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 모델 검증\n",
    "Training set과 Test set각각에서 모델의 accuracy를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset에서 성능 검증\n",
    "train_acc = sum(svm.predict(X_train) == y_train) / len(y_train)\n",
    "print('Train set accuracy: %.2f%%' % (train_acc*100))\n",
    "# Test dataset에서 성능 검증\n",
    "test_acc = sum(svm.predict(X_test) == y_test) / len(y_test)\n",
    "print('Test set accuracy: %.2f%%' % (test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Decision Boundary\n",
    "train 데이터셋과 test 데이터셋 각각을 이용하여 2개의 scatter plot을 그리고 그 위에 학습된 가중치를 이용하여 결정경계를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 데이터셋에서 Decision Boundary 시각화\n",
    "X_train = np.array(X_train)\n",
    "x1_min, x1_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "x2_min, x2_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "# Decision Boundary를 표시하기 위해\n",
    "# x1_min부터 x1_max까지 0.02간격으로, x2_min부터 x2_max까지 0.02간격의 모든 점의 좌표를 반환\n",
    "xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.02),\n",
    "                       np.arange(x2_min, x2_max, 0.02))\n",
    "\n",
    "Z = svm.predict(np.array([xx1.ravel(), xx2.ravel()]).T) # predict all datapoint from mashgrid\n",
    "Z = Z.reshape(xx1.shape)\n",
    "plt.contourf(xx1, xx2, Z, alpha=0.3) # 범위를 색으로 표시\n",
    "\n",
    "# 그래프로 표현\n",
    "plt.scatter([i[0] for idx, i in enumerate(X_train) if y_train[idx] == 0], \n",
    "            [i[1] for idx, i in enumerate(X_train) if y_train[idx] == 0],\n",
    "            label='class 0', marker='o')\n",
    "plt.scatter([i[0] for idx, i in enumerate(X_train) if y_train[idx] == 1], \n",
    "            [i[1] for idx, i in enumerate(X_train) if y_train[idx] == 1],\n",
    "            label='class 1', marker='s')\n",
    "\n",
    "# 그래프로 표현\n",
    "plt.title('Training set')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### Test 데이터셋에서 Decision Boundary 시각화\n",
    "X_test = np.array(X_test)\n",
    "x1_min, x1_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1\n",
    "x2_min, x2_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1\n",
    "xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.02),\n",
    "                       np.arange(x2_min, x2_max, 0.02))\n",
    "Z = svm.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "Z = Z.reshape(xx1.shape)\n",
    "plt.contourf(xx1, xx2, Z, alpha=0.3)\n",
    "\n",
    "# 그래프로 표현\n",
    "plt.scatter([i[0] for idx, i in enumerate(X_test) if y_test[idx] == 0], \n",
    "            [i[1] for idx, i in enumerate(X_test) if y_test[idx] == 0],\n",
    "            label='class 0', marker='o')\n",
    "plt.scatter([i[0] for idx, i in enumerate(X_test) if y_test[idx] == 1], \n",
    "            [i[1] for idx, i in enumerate(X_test) if y_test[idx] == 1],\n",
    "            label='class 1', marker='s')\n",
    "# 그래프로 표현\n",
    "plt.title('Testing set')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) SVM with soft margin\n",
    "\n",
    "위에서 실습한 선형 SVM 모델은 hard margin을 사용했습니다.이번에는 soft margin을 사용하여 학습 및 검증을 진행해 봅니다.\n",
    "\n",
    "**세부 구현 사항:**\n",
    "- kernel은 'linear'\n",
    "- Soft margin을 사용하기 위해 하이퍼파라미터 C=0.05으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### <your code> training with train dataset\n",
    "##### <your code> training with train dataset\n",
    "\n",
    "# Train dataset에서 성능 검증\n",
    "train_acc = sum(svm.predict(X_train) == y_train) / len(y_train)\n",
    "print('Train set accuracy: %.2f%%' % (train_acc*100))\n",
    "# Test dataset에서 성능 검증\n",
    "test_acc = sum(svm.predict(X_test) == y_test) / len(y_test)\n",
    "print('Test set accuracy: %.2f%%' % (test_acc*100))\n",
    "\n",
    "### 데이터셋에서 Decision Boundary 시각화\n",
    "X_train = np.array(X_train)\n",
    "x1_min, x1_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "x2_min, x2_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "# Decision Boundary를 표시하기 위해\n",
    "# x1_min부터 x1_max까지 0.02간격으로, x2_min부터 x2_max까지 0.02간격의 모든 점의 좌표를 반환\n",
    "xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.02),\n",
    "                       np.arange(x2_min, x2_max, 0.02))\n",
    "\n",
    "Z = svm.predict(np.array([xx1.ravel(), xx2.ravel()]).T) # predict all datapoint from mashgrid\n",
    "Z = Z.reshape(xx1.shape)\n",
    "plt.contourf(xx1, xx2, Z, alpha=0.3) # 범위를 색으로 표시\n",
    "\n",
    "# 그래프로 표현\n",
    "plt.scatter([i[0] for idx, i in enumerate(X_train) if y_train[idx] == 0], \n",
    "            [i[1] for idx, i in enumerate(X_train) if y_train[idx] == 0],\n",
    "            label='class 0', marker='o')\n",
    "plt.scatter([i[0] for idx, i in enumerate(X_train) if y_train[idx] == 1], \n",
    "            [i[1] for idx, i in enumerate(X_train) if y_train[idx] == 1],\n",
    "            label='class 1', marker='s')\n",
    "# 그래프로 표현\n",
    "plt.title('Training set')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### Test 데이터셋에서 Decision Boundary 시각화\n",
    "X_test = np.array(X_test)\n",
    "x1_min, x1_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1\n",
    "x2_min, x2_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1\n",
    "xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.02),\n",
    "                       np.arange(x2_min, x2_max, 0.02))\n",
    "Z = svm.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "Z = Z.reshape(xx1.shape)\n",
    "plt.contourf(xx1, xx2, Z, alpha=0.3)\n",
    "\n",
    "# 그래프로 표현\n",
    "plt.scatter([i[0] for idx, i in enumerate(X_test) if y_test[idx] == 0], \n",
    "            [i[1] for idx, i in enumerate(X_test) if y_test[idx] == 0],\n",
    "            label='class 0', marker='o')\n",
    "plt.scatter([i[0] for idx, i in enumerate(X_test) if y_test[idx] == 1], \n",
    "            [i[1] for idx, i in enumerate(X_test) if y_test[idx] == 1],\n",
    "            label='class 1', marker='s')\n",
    "# 그래프로 표현\n",
    "plt.title('Testing set')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Discussion\n",
    "\n",
    "**1) 이전 실습에서 사용된 Perceptron 과 SVM의 차이점에 대해 설명해보세요.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[답변작성]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) 선형 SVM에서의 hard margin과 soft margin의 차이를 설명하고, 본 실습 결과를 바탕으로 주어진 데이터셋에 더 적절한 방법은 무엇인지 설명하세요.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[답변작성]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P4.B:  SVM을 이용하여 비선형 분류 (7점)\n",
    "\n",
    "### 학습목표\n",
    "- SVM 모델을 이용하여 **선형 분류 불가능**한 데이터를 분류하는 분류기를 학습 할 수 있다.\n",
    "- Scikit-Learn을 이용하여 모델 학습, 검증 과정을 구현할 수 있다.\n",
    "\n",
    "### 실습내용\n",
    "Scikit-Learn의 SVM을 이용하여 선형 분류가 불가능한 데이터셋을 분류하는 분류기를 학습해봅니다. \n",
    "\n",
    "실습은 다음 순서로 진행됩니다.\n",
    "- 1) 데이터셋 loading\n",
    "- 2) SVM 모델 구현, 학습, 검증, Decision Boundary 시각화 **<직접 구현>**\n",
    "- 3) Discussion\n",
    "\n",
    "**이번 실습에서 여러분은 `2) SVM 모델 구현, 학습, 검증, Decision Boundary 시각화` 부분의 코드를 직접 작성합니다.**\n",
    "\n",
    "앞으로 대부분의 실습도 위와 같은 순서로 진행됩니다. 이번 실습을 통해 각 부분의 코드를 이해하고 다음 실습에 참고하도록합니다.\n",
    "\n",
    "\n",
    "### 점수\n",
    "- Trainset과 Testset 정확도 95% 이상: 7점\n",
    "- Trainset과 Testset 정확도 94% 이상: 6점\n",
    "- Trainset과 Testset 정확도 93% 이상: 5점\n",
    "- 본 실습과제에서는 Discussion 성실도에 따른 감점 최대 2점\n",
    "\n",
    "\n",
    "`.ipynb 파일과 함께 .html 파일 (File -> export as -> HTML)도 함께 제출하세요. 하나만 제출할시 감점이 있습니다.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 데이터셋 loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일을 읽기\n",
    "dataset = np.loadtxt(\"xor_dataset.csv\", delimiter=\",\")\n",
    "X = dataset[:, :2]\n",
    "y = dataset[:, 2].astype(int)\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Split, 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤시드 설정\n",
    "random.seed(123)\n",
    "\n",
    "# 데이터 랜덤 셔플\n",
    "idx = list(range(len(X)))\n",
    "random.shuffle(idx)\n",
    "\n",
    "# 앞 160개 까지는 학습용으로 뒤 40개는 테스트용으로 split\n",
    "X_train = [X[i] for i in idx[:160]]\n",
    "y_train = [y[i] for i in idx[:160]]\n",
    "X_test = [X[i] for i in idx[160:]]\n",
    "y_test = [y[i] for i in idx[160:]]\n",
    "\n",
    "# 데이터셋 plot\n",
    "plt.scatter([i[0] for idx, i in enumerate(X_train) if y_train[idx] == 1], \n",
    "            [i[1] for idx, i in enumerate(X_train) if y_train[idx] == 1],\n",
    "            marker='o')\n",
    "\n",
    "plt.scatter([i[0] for idx, i in enumerate(X_train) if y_train[idx] == -1], \n",
    "            [i[1] for idx, i in enumerate(X_train) if y_train[idx] == -1],\n",
    "            marker='s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) SVM 모델 구현, 학습, 검증\n",
    "\n",
    "비선형 커널을 사용하는 SVM모델을 이용하여 위 데이터셋을 분류하는 분류기를 학습합니다.\n",
    "\n",
    "- 선형 SVM 모델과 동일하게 SVC 모듈을 사용하지만, 'kernel' 파라미터를 조절하여 비선형 SVM 모델을 구현합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선형 불가능한 문제\n",
    "![](https://blog.kakaocdn.net/dn/1hyfi/btqwdhw53hZ/s33aTg9XF2ZDnoPkKs4IT0/img.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림은 선형분리가 불가능한 데이터셋을 어떻게 커널 SVM이 분류하게 하는지 보여줍니다.\n",
    "위 데이터셋은 2차원 평명상에서는 선형 분리가 불가능하지만, 오른쪽 처럼 3차원에서 보면 한 평면으로 분리가 가능합니다.\n",
    "여기서 커널은 2차원 데이터를 3차원으로 매핑하는 함수를 의미하며, 커널의 모양에 따라 linear(선형), poly(다항), rbf(가우시안), sigmoid(시그모이드) 등으로 구분됩니다.\n",
    "3차원상의 데이터를 구분하는 평면을 결정 평면이라고 합니다. \n",
    "\n",
    "그러나 커널은 단순히 2차원 데이터를 3차원으로 만 매핑하는것이 아니라 n차원 데이터를 n차원보다 높음 m차원으로 매핑 할 수 있습니다.\n",
    "따라서 3차원 이상의 차원에서의 결정 경계를 결정 초평면이라고합니다.\n",
    "\n",
    "정리하자면 커널 SVM은 다음과정을 통해 학습됩니다.\n",
    "- 1) 커널함수를 이용해 데이터를 초공간으로 매핑\n",
    "- 2) 매핑된 초공간에서 margin이 최대화 되는 결정 초평면 탐색\n",
    "\n",
    "하지만 커널함수를 모든 데이터셋에 적용하게 되면 데이터셋이 커질수록 계산복잡도가 증가하게 됩니다. 따라서 우리는 모든 데이터셋에 적용하는것이 아니라 꼼수(Trick)을 써서 커널함수를 적용하게 됩니다. 이를 커널 트릭(Kernel Trick)이라고 합니다.\n",
    "\n",
    "**더 읽을 거리**\n",
    "- [커널 함수와 커널 트릭](https://en.wikipedia.org/wiki/Kernel_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 실습에서 제공하는 데이터는 선형커널로는 분류가 불가능합니다. 위에서 설명한 커널들로 아래 코드를 변경하면서 **최적의 커널 및 하이퍼파라메터**를 찾아보세요.\n",
    "\n",
    "사용가능한 파라미터는 다음과 같습니다.\n",
    "- **kernel**: 'rbf', 'poly', 'sigmoid' 등의 커널 종류를 선택합니다.\n",
    "- **C**: 규제 파라미터로서 값이 작을 수록 찾아지는 결정 경계의 마진이 커집니다.\n",
    "- **gamma**: rbf, poly, sigmoid 커널에 사용되는 값으로 커널의 영향력을 나타냅니다. 값이 커질 수록 커널의 영향력이 커저 결정경계는 더 샘플에 가까워지고 구불구불하게 됩니다.\n",
    "\n",
    "이외의 파라미터는 오른쪽 사이트를 참고하세요.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### <your code> SVC module for Non-linear SVM\n",
    "##### <your code> training with train dataset\n",
    "\n",
    "# Train dataset에서 성능 검증\n",
    "train_acc = sum(svm.predict(X_train) == y_train) / len(y_train)\n",
    "print('Train set accuracy: %.2f%%' % (train_acc*100))\n",
    "\n",
    "# Test dataset에서 성능 검증\n",
    "test_acc = sum(svm.predict(X_test) == y_test) / len(y_test)\n",
    "print('Test set accuracy: %.2f%%' % (test_acc*100))\n",
    "\n",
    "### 데이터셋에서 Decision Boundary 시각화\n",
    "x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.02),\n",
    "                       np.arange(x2_min, x2_max, 0.02))\n",
    "Z = svm.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "Z = Z.reshape(xx1.shape)\n",
    "plt.contourf(xx1, xx2, Z, alpha=0.3)\n",
    "\n",
    "# 그래프로 표현\n",
    "plt.scatter([i[0] for idx, i in enumerate(X) if y[idx] == 1], \n",
    "            [i[1] for idx, i in enumerate(X) if y[idx] == 1],\n",
    "            label='class 0', marker='x')\n",
    "plt.scatter([i[0] for idx, i in enumerate(X) if y[idx] == -1], \n",
    "            [i[1] for idx, i in enumerate(X) if y[idx] == -1],\n",
    "            label='class 1', marker='s')\n",
    "## testset 강조\n",
    "plt.scatter([i[0] for idx, i in enumerate(X_test) if y_test[idx] == 1],\n",
    "            [i[1] for idx, i in enumerate(X_test) if y_test[idx] == 1],\n",
    "            label='class 0 (test)', facecolors='none', edgecolor='black', s=100, marker='o')\n",
    "plt.scatter([i[0] for idx, i in enumerate(X_test) if y_test[idx] == -1],\n",
    "            [i[1] for idx, i in enumerate(X_test) if y_test[idx] == -1],\n",
    "            label='class 1 (test)', facecolors='none', edgecolor='red', s=100, marker='o')\n",
    "\n",
    "\n",
    "# 그래프로 표현\n",
    "plt.title('Dataset')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) 학습, 테스트 정확도는 얼마인가요? (위 숫자를 복사하세요.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training:  ???%\n",
    "- Test ???%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) 본 실습에서 최종적으로 사용한 커널은 무엇인지 쓰고, 다른 커널들과 비교했을 때의 장단점(각 커널의 특성을 고려하여)을 설명하세요.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[답변작성]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
